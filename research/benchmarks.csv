Name,Variant,Implement,Estimated no. of models,Description,Domains (comma separated),superseeded_by
AI2D,,yes (image),1-10,"AI2D (AI2 Diagram Dataset) is a diagram understanding benchmark that evaluates whether models can interpret educational diagrams and answer questions grounded in visual structure and labels. It targets the ability to read arrows, parts, legends, and spatial relationships, often requiring integration of the diagram’s visual semantics with accompanying text. AI2D is a strong match when your task description includes “understand a labeled diagram,” “answer questions about processes shown in a figure,” “explain relationships in a schematic,” or “science education visual reasoning.” Vector-search matches: diagram QA, schematic understanding, labeled figure interpretation, and science diagram comprehension. Failure modes include ignoring diagram topology, misreading labels, and substituting world knowledge for the actual diagram evidence.","diagrams, multimodal understanding, education, science reasoning, vision-language",
AIME,2025,yes (math; hard),80-200,"AIME 2025 is a competitive mathematics evaluation drawn from a high-signal olympiad-style exam format that stresses multi-step reasoning, algebraic manipulation, combinatorics, number theory, and geometry in a concise answer format. It is frequently used as a proxy for “hard math reasoning” because the problems typically require creative problem decomposition, nontrivial derivations, and careful handling of edge cases, while still producing objectively checkable final answers. AIME-style evaluation is most relevant when the target workload involves symbolic reasoning, precise numeric outcomes, and multi-stage logical construction rather than open-ended explanation quality. Vector-search matches include: contest math, olympiad reasoning, multi-step derivation, numeric answer correctness, and algebraic/geometry problem solving. Common failure modes: premature heuristic guessing, algebra slips, losing track of constraints, and producing plausible-looking but incorrect arithmetic conclusions.","mathematics, competition math, reasoning, algebra, geometry, number theory, combinatorics",
API-Bank,,,1-10,"API-Bank is a benchmark suite for API/tool calling where the agent must solve tasks by interacting with a bank of APIs in a structured manner. It targets realistic tool-based workflows: choosing which API endpoint to call, forming arguments correctly, interpreting responses, and chaining calls to reach a user goal. API-Bank is useful for testing whether a model can behave like a reliable “program orchestrator” under schema constraints. Vector-search matches: API orchestration benchmark, tool chaining, structured arguments, endpoint selection, and reliable tool-augmented assistants.","tool use, API orchestration, structured output, agents",
APPS,,,1-10,"APPS is a code-generation benchmark oriented around competitive-programming-style tasks that require writing complete programs to solve algorithmic problems, often with more complexity and variety than short function-only benchmarks. It is relevant when you care about end-to-end program synthesis, handling input/output formats, and algorithmic reasoning under constraints. Vector-search matches: competitive programming benchmark, full program generation, algorithmic problem solving, and IO-format correctness under tests.","code generation, algorithms, competitive programming, full programs",
ARC,Challenge,,1-10,"The AI2 Reasoning Challenge (ARC) is a science-question benchmark that measures reasoning and knowledge in grade-school science, usually split into Easy and Challenge subsets. The Challenge subset is used as a stronger reasoning signal: questions are designed to be harder to answer by shallow retrieval or pattern matching alone. ARC is useful for evaluating structured multiple-choice reasoning under limited context, often serving as a long-lived baseline for general model competence. Vector-search matches: grade-school science QA, multiple-choice reasoning, science commonsense, and baseline reasoning benchmarks.","science QA, reasoning, multiple-choice, education",
ARC-AGI,,,1-10,"ARC-AGI is an abstract reasoning benchmark focused on general intelligence via pattern discovery and transformation, typically framed as learning a latent rule from a few input-output examples and applying it to a new input. It targets systematic generalization: can a model infer the underlying transformation rule, represent it robustly, and apply it correctly without overfitting to surface features? ARC-AGI is particularly relevant for tasks described as “induction from few examples,” “symbolic pattern reasoning,” “grid-based transformations,” “program-like rule inference,” and “compositional generalization.” In vector-search terms it matches: abstraction, algorithmic reasoning, few-shot rule learning, and robust generalization under distribution shift. Failure modes include brittle heuristics, memorization of common patterns, and inability to compose multiple sub-rules into a single transformation program.","abstract reasoning, few-shot generalization, induction, symbolic pattern learning",ARC-AGI v2
ARC-AGI,v2,maybe (hard),1-10,"ARC-AGI v2 is a successor-style iteration that aims to make the abstract reasoning evaluation harder, more robust, and less vulnerable to shortcut strategies. It is appropriate when you want a stronger separation between models that can genuinely infer and implement latent rules versus models that rely on pattern libraries or partial heuristics. Vector-search matches include: harder ARC, improved abstraction benchmark, anti-shortcut induction, and robust program-like generalization. Typical failure modes it exposes are the same family as ARC-AGI, but under conditions where naive pattern matching is less effective and deeper rule composition or algorithmic search is required.","abstract reasoning, robust generalization, induction, compositionality",
AgentBench,,,1-10,"AgentBench is a benchmark framework intended to evaluate general-purpose agents across diverse tasks that require planning, tool usage, and multi-step execution rather than single-shot answering. It is commonly used when you want a broad “agent capability profile” across multiple categories, including task decomposition, memory usage, tool selection, and robustness to intermediate failures. For vector-search matching, AgentBench aligns with phrases like general agent benchmark, multi-domain agent evaluation, tool-using agents, planning-and-execution tests, and long-horizon task completion.","agents, planning, tool use, multi-step execution, robustness",
BFCL,v3,,30-80,"BFCL v3 (Berkeley Function Calling Leaderboard v3) is a tool-use and function-calling evaluation designed to measure whether a model can reliably translate natural-language intent into correct, executable API calls. The benchmark is fundamentally about structured action selection under constraints: given a user request and a catalog of available functions (with names, signatures, parameter schemas, and natural-language documentation), the model must (1) choose the right function(s), (2) generate arguments that satisfy the schema, (3) map entities and values from the request into the correct fields, and (4) preserve subtle constraints such as required parameters, allowed enums, and valid types. BFCL is particularly valuable when your production task involves JSON tool calls, OpenAPI-like interfaces, agent frameworks that call external services, or “function routing” problems where a small formatting or argument mistake breaks execution. In vector-search terms, BFCL v3 matches queries like: function calling, API invocation, schema adherence, structured output, tool arguments, parameter grounding, multi-tool planning, and robust execution formatting. Typical failure modes BFCL helps surface include hallucinated parameters, wrong function selection, partial argument filling, incorrect nesting/typing, and brittle formatting that would not survive strict parsers.","function calling, tool use, structured output, API invocation, schema adherence",BFCL v4
BFCL,v4,yes (tool),30-80,"BFCL v4 (Berkeley Function Calling Leaderboard v4) is a holistic agentic evaluation benchmark for tool-use and function-calling that measures an LLM's ability to perform structured actions in complex, real-world agentic scenarios beyond isolated calls. Building on prior versions, it stresses multi-hop web search with injected API errors like 503s or rate limits, long-term memory management across key-value, vector, and recursive summarization backends, format sensitivity to varying documentation styles, and stateful multi-turn reasoning with dynamic decision-making and abstention. The model must (1) select appropriate tools amid noisy environments, (2) handle serial/parallel calls with schema-compliant arguments, (3) maintain context via store/retrieve/update operations in domains like healthcare or finance, (4) recover from failures while preserving constraints on enums, types, and nesting, and (5) generate robust outputs that survive strict AST-based or state-transition verification. BFCL v4 shines for production systems involving agent frameworks, RAG pipelines with external APIs, memory-augmented assistants, or end-to-end workflows where lapses in recall, error handling, or formatting cascade into total breakdowns. In vector-search terms, BFCL v4 aligns with queries like: agentic tool-calling, multi-hop search robustness, memory backend integration, error recovery planning, semantic retrieval grounding, long-horizon state tracking, and resilient execution under perturbations. Common pitfalls it exposes include over-aggressive memory pruning, hallucinated retrievals from semantic drift, brittle parsing on format variants, premature tool invocation without clarification, incomplete error recovery loops, and context loss in extended interactions that evade single-turn benchmarks","function calling, tool use, structured output, API invocation, schema adherence",
BIG-Bench,,,1-10,"BIG-Bench is a broad benchmark suite designed to probe a wide range of language-model abilities across many tasks, including reasoning, knowledge, translation, algorithmic thinking, and creative or social-language behaviors. It is useful as a ‘capability map’ rather than a single score: the value is in the diversity of tasks that can reveal uneven strengths and weaknesses, emergent behaviors, and brittleness across different task formats. BIG-Bench is a strong fit when you want breadth coverage, when your evaluation question is “what can this model do across a wide variety of behaviors?”, or when you need a task library that spans many modes (classification, generation, reasoning puzzles, structured prompts). Vector-search matches: broad evaluation suite, multitask benchmarking, capability profiling, and emergent behavior tracking. Failure modes it highlights include task-specific brittleness, prompt sensitivity, and uneven competence across superficially similar tasks.","general knowledge, reasoning, language understanding, multitask evaluation, creativity",BIG-Bench Hard
BIG-Bench,Hard,yes (hard),1-10,"BIG-bench hard (BBH) is a curated subset of BIG-Bench that focuses on the more difficult tasks intended to better discriminate strong models and reduce ceiling effects. It is commonly used when models saturate easier benchmarks and you need a harder stress test for reasoning, instruction following, and structured thinking. BBH is a strong match for tasks described as “hard reasoning benchmark,” “difficult multi-step puzzles,” “strong separation between SOTA models,” and “beyond basic QA.” Vector-search matches: BBH, difficult reasoning suite, anti-ceiling evaluation, and multi-step structured tasks. Failure modes often include shallow heuristics that work on easy sets but collapse on harder variants, and inability to maintain multi-step consistency.","reasoning, hard tasks, instruction following, structured thinking, evaluation",
BigCodeBench,,maybe (coding; many languages),10-30,"BigCodeBench is a code generation benchmark with a public leaderboard that evaluates models on function-level programming tasks spanning many real libraries, designed to measure practical code-writing ability beyond toy examples. The benchmark framing supports evaluating both completion-style and instruction-style code generation, making it useful when the task description includes “write a function using real APIs,” “library-specific coding,” “implement behavior described in text,” or “generalize across diverse programming ecosystems.” For vector-search matching, BigCodeBench is a good candidate when you want a broad, library-diverse code generation signal that complements repo-level patch benchmarks like SWE-bench by focusing on self-contained function implementations across many packages.","code generation, programming libraries, function implementation, instruction-to-code",
BrowseComp,,yes (tool),10-30,"BrowseComp is a browsing-agent benchmark aimed at measuring the skill of locating hard-to-find information on the live web via iterative search, page selection, evidence extraction, and convergence on a short final answer. Unlike classic closed-book QA, the benchmark is about information foraging: the agent must explore, decide what to trust, and stitch together evidence from multiple pages that may not be obviously relevant from the initial query. The tasks are crafted so that naive single-query retrieval often fails; success typically requires reformulating searches, opening multiple sources, and resolving ambiguities. BrowseComp is especially suitable for evaluating research agents, web automation assistants, and retrieval-augmented systems where the “hard part” is not language generation but strategic exploration and verification. Vector-search phrases that map strongly include: web research benchmark, browsing agent evaluation, iterative search strategy, source triangulation, evidence-based answering, hard-to-retrieve facts, and long-horizon information gathering.","web browsing, research agents, information retrieval, source credibility, evidence synthesis, agentic search",
BrowseComp,Plus,,1-10,"BrowseComp-Plus is an extension-style benchmark direction that preserves the core “browsing agent” loop while aiming to increase difficulty, breadth, and realism of web research tasks. It is relevant when you want BrowseComp-like evaluation but with more challenging retrieval conditions, more adversarial or noisy web content, and more emphasis on robust search planning and evidence reconciliation across sources. Vector-search alignment: browsing benchmark harder, web research under noise, long-horizon search plans, robust page selection, and evidence stitching when sources disagree.","web browsing, research agents, robust retrieval, adversarial web content",
BrowseComp,zh,,1-10,"A high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web, consisting of 289 multi-hop questions spanning 11 diverse domains including Film & TV, Technology, Medicine, and History. Questions are reverse-engineered from short, objective, and easily verifiable answers, requiring sophisticated reasoning and information reconciliation beyond basic retrieval. The benchmark addresses linguistic, infrastructural, and censorship-related complexities in Chinese web environments","web browsing, research agents, information retrieval, source credibility, evidence synthesis, agentic search"," research agents"," information retrieval"," source credibility"," evidence synthesis"," agentic search"
CaseLaw,v2,yes (law),10-30,"CaseLaw from Vals AI (v2) is a legal-domain benchmark designed to evaluate legal reasoning and legal research behaviors grounded in case law. It targets tasks where success depends on correctly interpreting judicial opinions, extracting holdings and relevant facts, applying precedent, and producing answers that reflect the structure and constraints of legal analysis rather than generic summarization. CaseLaw is a strong fit when your task description includes: legal research assistant, case interpretation, precedent application, issue-spotting in opinions, legal argument construction, and citation-grounded reasoning. Vector-search matches include: case law QA, legal reasoning benchmark, judicial opinion comprehension, precedent analysis, and lawyer-style synthesis. Common failure modes include mischaracterizing holdings, mixing facts across cases, overconfident assertions without textual grounding, and failing to distinguish dicta from binding reasoning.","law, legal reasoning, case analysis, legal research, information extraction",
ChartQA,,,1-10,"ChartQA evaluates whether models can interpret charts and answer questions that require reading axes, legends, and data points, often involving numeric comparison or aggregation. It is relevant for analytics assistants and report-reading agents that must reliably ground answers in plotted data rather than narrative text. Vector-search matches: chart understanding benchmark, data visualization QA, plot-grounded numeric reasoning, and axis/legend interpretation.","charts, data visualization, multimodal reasoning, quantitative QA",
CodeContests,,,1-10,"CodeContests is an evaluation direction for contest-style coding problems, used to measure algorithmic reasoning and program synthesis at scale. It is relevant for models that must solve complex problems with careful algorithm selection, edge-case handling, and strict correctness under hidden tests. Vector-search matches: contest coding evaluation, algorithmic synthesis benchmark, hard programming problems, and test-driven correctness.","code generation, algorithms, competitive programming, program synthesis",
CommonsenseQA,,,1-10,"CommonsenseQA is a commonsense reasoning benchmark in multiple-choice format that tests everyday knowledge and conceptual associations. It is used to evaluate whether models can choose the correct answer among distractors that are semantically plausible but wrong. Vector-search matches: commonsense multiple-choice benchmark, conceptual association reasoning, everyday knowledge inference, and distractor-resistant selection.","commonsense, reasoning, multiple-choice",
CorpFin,v2,yes (finance),10-30,"CorpFin from Vals AI (v2) is a finance-domain benchmark focused on corporate finance reasoning and analysis. It targets tasks that look like professional finance work: interpreting financial statements, reasoning about valuation concepts, analyzing scenarios involving capital structure, cash flows, accounting implications, and making quantitatively grounded judgments. CorpFin is a strong match when the task description includes: investment analysis, CFO-style reasoning, corporate finance Q&A, accounting-aware numeric reasoning, and structured finance explanations that must be consistent with finance definitions and constraints. Vector-search matches: corporate finance benchmark, valuation reasoning, financial-statement interpretation, accounting logic, and quantitative finance explanation. Failure modes include mixing accounting concepts, producing inconsistent computations, and giving plausible narrative finance answers without coherent quantitative grounding.","finance, corporate finance, accounting, quantitative reasoning, analysis",
CritPt,,yes (physics),30-80,"CritPt is a frontier-style scientific reasoning benchmark that targets research-grade physics problem solving, designed to probe whether models can sustain deep, technical reasoning and produce checkable final outputs. Instead of focusing on short textbook questions, it emphasizes complex, multi-stage challenges that resemble miniature research workflows: deriving equations, managing assumptions, translating a conceptual setup into mathematical form, and producing final results in a canonical form that can be verified (often numerically, symbolically, or via code). CritPt is a strong match for tasks described as advanced physics reasoning, derivations, symbolic manipulation, computational physics, or “research-like” problem decomposition where the model must be both rigorous and precise. It is especially useful when you want to measure whether a model’s chain of reasoning is not only fluent but also *structurally correct*, and whether it can avoid subtle but fatal mistakes that are common in complex math/physics. Vector-search match phrases include: frontier physics, research reasoning, symbolic verification, derivation-heavy tasks, computational checks, and precision-required scientific problem solving.","physics, scientific reasoning, mathematics, symbolic computation, research workflows",
DROP,,yes (combined reasoning),1-10,"DROP (Discrete Reasoning Over Paragraphs) is a reading-comprehension benchmark that tests whether models can perform discrete, symbolic reasoning grounded in a passage. It is designed to require operations like counting, addition/subtraction, comparison, sorting events, and extracting spans—often combining multiple facts from a paragraph into a computed or logically derived answer. DROP is a strong fit when the task you care about is “reasoning from text with numbers and entities,” especially in contexts like reports, sports summaries, timelines, or any narrative where the answer is not a single copied phrase but a derived result from multiple mentions. Vector-search matches: paragraph-grounded discrete reasoning, numerical reading comprehension, multi-hop extraction + computation, and text-based counting. Failure modes include copying a plausible span instead of computing, losing track of referents, and failing to aggregate information across sentences.","reading comprehension, numerical reasoning, information extraction, multi-hop reasoning",
DS-1000,,maybe (coding; data science),1-10,"DS-1000 is a code benchmark centered on data science programming tasks that require using common libraries and writing code that manipulates data, performs analysis, or produces correct transformations. It is a strong fit when the target application is a data-analysis assistant rather than a pure algorithms solver. Vector-search matches: data science code generation, pandas/numpy programming benchmark, library-usage correctness, and analysis-script synthesis.","data science, code generation, library use, analysis scripts",
DocVQA,,maybe (image),1-10,"DocVQA is a document visual question answering benchmark that evaluates whether models can answer questions grounded in document images, often requiring OCR-like reading plus layout understanding and reasoning. It is essential for evaluating assistants that process invoices, forms, reports, and scanned documents where the “meaning” is distributed across text and layout. Vector-search matches: document QA, OCR + layout reasoning, form understanding, invoice extraction with questions, and visually grounded document comprehension.","documents, OCR, layout understanding, multimodal QA",
FEVER,,,1-10,"FEVER is a fact verification benchmark where models must determine whether a claim is supported, refuted, or not verifiable based on evidence. It targets grounded reasoning and evidence selection, making it valuable for evaluating hallucination resistance and truthfulness under an evidence requirement. Vector-search matches: claim verification, evidence-based classification, fact-checking benchmark, grounded entailment, and retrieval + verification workflows.","fact checking, verification, entailment, retrieval, grounding",
FrontierMath,,,1-10,"FrontierMath is a frontier-level mathematics benchmark designed to push beyond typical contest math by targeting problems that require deeper mathematical maturity, careful formal reasoning, and nontrivial multi-step derivations. It is relevant when the evaluation goal is “math at the edge of current model capability,” including tasks where minor algebraic slips are fatal and where the solution requires sustained symbolic reasoning rather than short tricks. FrontierMath is a strong match for descriptions like research-grade math, advanced problem solving, rigorous derivations, and evaluation of mathematical depth. Vector-search terms include: advanced math reasoning, high difficulty mathematics, long derivations, symbolic precision, and proof-like thinking.","mathematics, advanced reasoning, symbolic manipulation, proof-like problem solving",
GAIA,,,10-30,"GAIA (General AI Assistants) is an agentic benchmark designed to evaluate “general AI assistants” on real-world questions that often require multi-step reasoning, web browsing, and tool use, rather than purely closed-book recall. The defining property is that tasks resemble what a capable assistant would do for a user: find specific information, follow a multi-hop trail, interpret sources, and produce a final grounded answer. GAIA is especially relevant when you want to test end-to-end assistant behavior (search, reading, synthesis, verification), including failure modes like stopping too early, citing wrong sources, or confusing similar entities. Vector-search matches: general assistant benchmark, web-enabled reasoning, multi-hop research tasks, tool-augmented QA, and realistic information-seeking workflows.","agents, web research, multi-hop reasoning, information retrieval, tool use",
GPQA,,maybe (general),200+,"GPQA stands for a high-difficulty, domain-expert crafted question set designed to stress-test scientific reasoning in closely controlled, subject-matter-specific contexts. Comprising 448 multiple-choice items authored by domain experts in biology, physics, and chemistry, GPQA challenges even seasoned researchers with questions that go beyond mere factual recall and toward genuine conceptual inference, problem decomposition, and cross-domain integration. The questions are engineered to be Google-proof, minimize luck, and resist superficial search strategies, thereby serving as a robust proxy for evaluating true knowledge synthesis, experimental reasoning, and the ability to apply canonical principles to novel scenarios. This benchmark is particularly well-suited for evaluating a model’s capacity for deep reasoning, precise scientific vocabulary usage, and structured justification of answers. Use cases include validating foundational scientific comprehension across STEM domains, stress-testing reasoning under constrained formats (e.g., limited context, single best answer), and benchmarking improvements in models intended for high-stakes scientific consultation, education, or research assistance. Typical evaluation signals include high dispersion of distractor options, requirement for precise justification, and the presence of subtler conceptual traps that differentiate expert-level understanding from surface familiarity. Ideal for: (1) rigorous scientific reasoning evaluation in biology, physics, and chemistry; (2) testing resilience to search-based shortcuts; (3) assessing the ability to generate coherent, domain-accurate explanations and justifications; (4) benchmarking for professional or academic-grade QA and tutoring systems in STEM.","biology, physics, chemistry",
GPQA,Diamond,yes (general),80-200,"GPQA Diamond is the gateway-to-expertise subset of GPQA, distilled to the 198 most challenging items that preserve Google-proof characteristics while intensifying the cognitive load. These questions are curated to require genuine scientific reasoning, deep domain knowledge, and the synthesis of concepts across multiple subfields of biology, physics, and chemistry. The Diamond variant is tailored for evaluating models’ capabilities in high-stakes scientific discourse, including rigorous justification, precise argumentation, and the ability to identify nuanced cases where standard heuristics fail. Because the content targets graduate-level comprehension, it serves as a stringent litmus test for true expertise—questions are designed so that only domain PhDs or equivalent-level reasoning are consistently reliable. Use cases include: (1) stress-testing advanced AI assistants intended for research advisors or doctoral-level mentorship, (2) benchmarking the limits of mechanistic reasoning and inference under tight constraints, (3) validating model behavior on complex, integrative science problems that require multi-step deduction, and (4) auditing models for potential overfitting to surface patterns rather than genuine scientific reasoning. Nature of evaluation signals: high reliance on internal consistency, demand for multi-step justification, and evaluation of the model’s ability to resist shallow pattern matching across disciplines.","biology, physics, chemistry",
GSM8K,,,200+,"GSM8K is a grade-school math word-problem benchmark that evaluates stepwise quantitative reasoning and basic arithmetic in natural language contexts. It is widely used because it exposes whether models can parse story problems, maintain intermediate quantities, and produce correct numeric outcomes without drifting. Vector-search matches: arithmetic word problems, step-by-step math reasoning, numeracy, and basic quantitative QA grounded in text. Failure modes include mis-parsing relationships, arithmetic slips, and losing track of intermediate results.","mathematics, arithmetic, word problems, reasoning, numeracy",
GSO,,maybe (coding),1-10,"GSO (Global Software Optimization) is presented as a benchmark/leaderboard of challenging software optimization tasks intended to evaluate SWE-agents on performance optimization rather than only functional correctness. It is a strong match when the task description is “speed up code,” “optimize runtime/throughput/latency,” “improve performance without breaking correctness,” “profiling-driven refactor,” or “optimization across real codebases,” where evaluation needs to capture measurable performance gains and correctness constraints. For vector-search matching, GSO is appropriate for performance engineering agents, compiler- or SIMD-aware optimization attempts, and cases where ‘making tests pass’ is insufficient because the primary requirement is *faster* execution under measurable metrics.","software optimization, performance engineering, SWE agents, benchmarking",
HLE,,yes (coding; hard),30-80,"HLE (Humanity’s Last Exam) is a frontier-style general evaluation designed to be substantially harder than standard knowledge tests, targeting deep reasoning, synthesis, and difficult question distributions that resist superficial pattern matching. It is typically used when conventional benchmarks are saturated and you want a harder signal that can still be measured consistently across top models. HLE is a strong match for tasks described as “frontier general reasoning,” “hard exam-like questions across many subjects,” “stress test for top models,” and “beyond MMLU/BBH difficulty.” Vector-search matches: extremely hard general benchmark, frontier evaluation, deep reasoning stress test, and anti-ceiling general knowledge evaluation. Common failure modes include confident hallucinations on unfamiliar material and brittle reasoning under high ambiguity or novelty.","general reasoning, hard evaluation, knowledge synthesis, frontier benchmarks",
HellaSwag,,,1-10,"HellaSwag is a commonsense reasoning benchmark framed as choosing the most plausible continuation of a situation, designed to test whether models can pick the option that best fits everyday physical and social commonsense. It is useful for evaluating narrative plausibility and implicit knowledge of how events unfold. Vector-search matches: commonsense inference, story continuation, everyday plausibility, and pragmatic reasoning about actions and consequences.","commonsense, reasoning, narrative plausibility, multiple-choice",
HotpotQA,,,1-10,"HotpotQA is a multi-hop question answering benchmark where answering correctly typically requires combining evidence from multiple documents or paragraphs. It targets the ability to chain facts, track entities across sources, and synthesize a final answer that depends on more than one retrieval hop. Vector-search matches: multi-hop QA, evidence chaining, cross-document reasoning, entity linking across passages, and retrieval-augmented reasoning evaluation.","multi-hop QA, retrieval, reasoning, evidence synthesis",
HumanEval,,maybe (coding; easy),200+,"HumanEval is a foundational code-generation benchmark focused on writing correct functions from natural-language docstrings and function signatures, typically evaluated by executing unit tests. It targets a compact but meaningful capability: can a model translate a specification into working code with correct logic, correct edge-case handling, and correct output types? HumanEval is especially useful for comparing general code synthesis ability across models in a way that is relatively easy to standardize and reproduce. In vector-search terms it matches: docstring-to-code, function synthesis, unit-test-based evaluation, code correctness, and basic algorithmic implementation. Failure modes include partial implementations that pass obvious cases but fail corner cases, incorrect assumptions about inputs, and “looks right” code that fails under execution.","code generation, program synthesis, unit tests, software correctness",HumanEval+
HumanEval+,,yes (coding),10-30,"HumanEval+ is an enhanced, more stringent variant of HumanEval that aims to reduce false positives by using stronger or expanded testing, making it harder for superficially correct solutions to slip through. It is valuable when your evaluation goal is robustness rather than just passing a small set of tests: you want to measure whether the model’s solution genuinely matches the intended specification across a broader input distribution. For vector-search matching, HumanEval+ aligns with: robust code evaluation, stronger test suites, correctness under adversarial inputs, and reduced overfitting to minimal tests. Typical failure modes revealed include brittle solutions, over-specialized hardcoding, and missing edge-case logic that basic HumanEval tests might not cover.","code generation, robust testing, program synthesis, correctness",
IF,,,80-200,"IF (often referred to via IFEval) is an instruction-following benchmark built around prompts whose requirements are *verifiable*, meaning an evaluation script can check whether the model actually complied with the stated constraints rather than merely sounding compliant. The benchmark is designed to measure precise adherence to explicit instructions (format constraints, inclusion/exclusion rules, ordering constraints, style constraints, and other checkable directives) in a way that reduces grader subjectivity and rewards controllable behavior. In vector-search terms, IF/IFEval is a strong fit when you need an evaluation signal for ‘did the model follow the rules exactly?’ rather than ‘was the answer generally good?’, especially for workloads like templated generation, policy-compliant responses, structured writing constraints, and agent tool-call argument correctness where instruction violations are the primary failure mode. Because it is typically single-turn and English-focused in its original formulation, it is most directly comparable for single-shot compliance and constraint satisfaction rather than multi-turn conversational reliability or multilingual instruction tracking.","instruction following, constraint satisfaction, controllability, compliance, structured output",Multi-IF
LegalBench,,yes (law),10-30,"LegalBench is a collaboratively built benchmark designed to measure legal reasoning in large language models across many legal tasks rather than a single narrow dataset, supporting evaluation of legal-domain competence in a structured, research-oriented way. It is also tracked by Vals AI as an open benchmark for evaluating legal reasoning, reinforcing its use as a common reference point for legal-domain model evaluation. For vector-search matching, LegalBench is a good candidate when your task description includes “legal reasoning,” “statute/case interpretation,” “contract analysis,” “legal QA with formal constraints,” “legal entailment,” or “domain-specific legal comprehension,” and you want a benchmark suite that covers multiple subskills rather than a single exam-like QA dataset.","law, legal reasoning, contracts, legal QA, classification, entailment",
LiveBench,,,10-30,"LiveBench is described as a challenging, contamination-free LLM benchmark with an associated paper/repository, designed to evaluate models using test data intended to be uncontaminated and objectively scored. It is a good fit when the task description calls for “general capability benchmarking with contamination controls,” “objective scoring,” or “broad evaluation across categories with minimized memorization risk,” especially when comparing frontier models where classic static test sets may be saturated. For vector-search matching, LiveBench aligns with the need for up-to-date, harder general evaluation signals and for measuring whether gains are robust rather than artifacts of training-data overlap.","general evaluation, contamination resistance, multi-task benchmarking, objective scoring",
LiveCodeBench,,maybe (coding; easy),80-200,"LiveCodeBench is presented as a holistic and contamination-resistant evaluation of code-capable models, with a focus on measuring code generation performance on continuously updated or time-aware coding tasks to reduce training contamination effects. The benchmark is a good fit when you want to evaluate code-writing ability under conditions where test items are newer or more carefully controlled against memorization, supporting comparisons that better approximate ‘generalization to unseen programming problems’. For vector-search matching, LiveCodeBench aligns with task descriptions like “fresh coding problems,” “contamination-aware code benchmark,” “competitive programming style code generation,” “code correctness under tests,” and “measure generalization over time,” especially when you want a benchmark identity that explicitly foregrounds contamination resistance.","code generation, contamination resistance, programming problems, generalization",
Long Context Reasoning,,yes (reasoning),30-80,"Artificial Analysis’ Long Context Reasoning (LCR) benchmark is described as a challenging evaluation that measures whether models can extract, reason about, and synthesize information from long-form documents ranging roughly from 10k to 100k tokens (tokenized with cl100k_base). It is particularly relevant for tasks where the primary difficulty is not world knowledge but *retrieval within context*: keeping track of scattered facts, resolving long-range dependencies, and composing an answer that integrates evidence distributed across a long input. For vector-search matching, this benchmark is a strong candidate when the task description includes “long document QA,” “multi-section report synthesis,” “cross-referencing across chapters,” “policy + appendix reasoning,” “10k+ token context,” or “needle-in-haystack plus reasoning,” where evaluation should punish hallucinated unsupported claims and reward grounded integration from the provided text.","long context, document reasoning, synthesis, reading comprehension, grounding, multi-document QA",
MATH,,,200+,"MATH is a mathematics benchmark centered on problems that require multi-step reasoning and structured solution development, often used to evaluate whether models can perform beyond simple arithmetic and into algebra, geometry, probability, number theory, and combinatorics. It is especially relevant when you want a benchmark that supports analyzing reasoning steps (even if the metric is final-answer accuracy) and that captures typical failure modes of math reasoning models: losing constraints, making algebraic mistakes, or failing to connect intermediate steps. Vector-search matches: multi-step math reasoning, competition-style math, symbolic calculation, and structured derivations. MATH is a good baseline when comparing general math capability across models, and it often acts as a ‘parent’ benchmark from which smaller diagnostic subsets are derived.","mathematics, reasoning, algebra, geometry, number theory, combinatorics",MATH 500
MATH,500,maybe (math; easy),30-80,"MATH-500 is a widely used subset of the MATH benchmark that provides a smaller, more evaluation-friendly slice while preserving the multi-step reasoning character of the full dataset. It is often used for quick iteration, regression testing, and comparative evaluation when running the full MATH suite is too slow or costly. For vector-search matching, MATH-500 fits descriptions like “fast math benchmark,” “small but hard math subset,” “quick eval for reasoning improvements,” and “math regression suite.” Failure modes mirror MATH: arithmetic slip cascades, misapplied theorems, and incomplete constraint handling, but the smaller size makes it particularly useful for repeated measurement in model development loops.","mathematics, reasoning, evaluation subset, regression testing",
MBPP,,,10-30,"MBPP (Mostly Basic Python Problems) is a code-generation benchmark centered on short Python programming tasks that resemble everyday scripting and small utility functions, typically judged by running tests. It is widely used because it targets practical coding skill: implementing simple algorithms, handling strings/lists/dicts, writing small transformations, and producing correct results with idiomatic Python. MBPP is a good fit when your task description is “basic programming,” “Python scripting,” “small function tasks,” or “coding interview-lite,” where you want a broad but accessible evaluation of code correctness. Vector-search matches: basic Python coding benchmark, short programs, test-driven function tasks, and practical scripting. Failure modes: off-by-one errors, incorrect handling of empty inputs, misunderstanding specification details, and producing code that is syntactically plausible but semantically wrong.","python coding, program synthesis, basic algorithms, scripting, unit tests",
MGSM,,,1-10,"MGSM is a multilingual grade-school math benchmark aimed at evaluating basic quantitative reasoning across languages, focusing on arithmetic word problems that require reading comprehension, numerical operations, and step-by-step reasoning. It is especially relevant when you want to measure whether a model’s math reasoning is language-agnostic: can it solve the same class of word problems in multiple languages without relying on English-specific patterns? In vector-search terms it matches: multilingual math word problems, arithmetic reasoning, cross-lingual quantitative QA, and basic numeracy evaluation. Typical failure modes include mistranslating quantities/units, mis-parsing relationships described in text, and arithmetic slips that cascade into wrong final answers.","multilingual, mathematics, arithmetic, word problems, quantitative reasoning",
MHPP,,yes (coding; hard),1-10,"MHPP (Mostly Hard Python Problems) is a function-level Python code generation benchmark intentionally designed to be *harder than the classic baselines* (notably HumanEval and MBPP) by concentrating on failure modes that easy-to-saturate benchmarks often under-measure. Instead of rewarding short, template-like solutions, MHPP emphasizes problems where the model must read a longer and more constraint-heavy natural-language specification, internalize nuanced restrictions, and then translate that spec into correct executable code that passes unit tests. The benchmark is particularly well aligned with real developer-assistant use cases where the user description is verbose, contains multiple interacting constraints, includes tricky edge cases, and where success requires multi-step reasoning rather than a single obvious algorithm. A defining theme is *specification fidelity*: MHPP surfaces whether a model can correctly interpret newly defined terms, respect “must/ must not” requirements, and avoid silently ignoring corner-case clauses that are easy to overlook in long problem statements. It is also useful for measuring robustness against ‘distraction’: when problem statements contain additional context or non-essential details, weaker models often latch onto superficial cues and implement the wrong behavior. In practice, MHPP acts like a “stress test” for code synthesis pipelines because it penalizes partial solutions that look plausible but fail on carefully constructed tests; it therefore aligns well with agentic coding systems that need reliable correctness, not just good-looking code. For vector search, MHPP is a strong match for task descriptions like: hard Python function synthesis, long spec to code, constraint-heavy programming tasks, unit-test-verified code generation, edge-case driven correctness, robust code reasoning, and evaluation beyond HumanEval/MBPP saturation.","python, code generation, program synthesis, unit testing, specification following, algorithmic reasoning",
MMLU,,,200+,"MMLU is a broad, multilingual, multitask benchmark designed to probe knowledge and reasoning across a large catalog of 57 subjects spanning STEM disciplines, humanities, social sciences, and professional domains. It evaluates language models on both breadth and depth of knowledge, spanning computation, science literacy, history, philosophy, economics, law, and more, to simulate real-world problem-solving across academic and professional contexts. The dataset emphasizes high-quality multiple-choice items that require more than rote memorization, including conceptual understanding, procedural familiarity, and the ability to map domain-specific concepts to practical contexts. MMLU is valuable for benchmarking general-purpose reasoning, adaptability across domains, and cross-domain transfer: a model strong in one subject should show reasonable performance in related areas. Typical use cases include: (1) broad-spectrum knowledge and reasoning evaluation for assistant systems used in teaching, tutoring, or research planning, (2) cross-domain capability assessment to inform alignment and safety considerations when handling diverse queries, (3) baseline measurement for language models before domain-specific fine-tuning, and (4) multilingual general knowledge benchmarking when deployed in multi-language settings. The original MMLU serves as a canonical, unified test bed, while its variants (Pro, Redux) refine scope, language coverage, and data quality as described below.","general knowledge, reasoning, multiple-choice, academic subjects",MMLU Redux
MMLU,Pro,maybe (general),80-200,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects. MMLU Pro is the professionally translated, multilingual, expansive version of the core MMLU benchmark, presenting approximately 15,908 multiple-choice questions per language across 14 languages. This variant preserves the same 57-subject structure—spanning STEM, humanities, social sciences, and professional domains—but elevates the linguistic reach and cultural nuance through professional translation, ensuring that language barriers do not suppress domain-accurate reasoning and knowledge. Pro emphasizes faithful translation fidelity, terminology consistency, and alignment with native-speaking knowledge expectations. It is especially well-suited for evaluating models in multilingual deployment scenarios where high-quality, language-aware reasoning is critical, such as international education tools, cross-language knowledge assistants, and research-assistance systems operating in non-English contexts. Use cases include: (1) assessing cross-linguistic reasoning parity and translation robustness, (2) benchmarking capabilities for multilingual tutoring assistants, (3) evaluating professional-domain comprehension across languages (engineering, medicine, law, etc.), and (4) comparing performance deltas between English-only baselines and multilingual deployments. The content fosters rigorous cross-language evaluation of domain knowledge with an emphasis on precise terminology and culturally aware interpretation.","general knowledge, reasoning, harder evaluation, multiple-choice, academic subjects",
MMLU,Redux,,10-30,"MMLU Redux offers an enhanced, quality-improved version of the original MMLU benchmark through manual re-annotation of questions to identify and correct errors present in the source dataset. The Redux edition thus provides more reliable evaluation metrics for language models by addressing dataset quality issues, ambiguous phrasing, and potential mislabeling that could otherwise inflate or obscure true model capabilities. Beyond error correction, Redux preserves the broad domain coverage—57 subjects across STEM, humanities, social sciences, and professional domains—and maintains the original bilingual/multilingual aspiration if language variants are included. This refinement is particularly valuable for researchers who prioritize measurement integrity, want to minimize noise in evaluation signals, and seek a more trustworthy basis for comparing model progress over time or across architectures. Ideal use cases include: (1) evaluation where reproducibility and label fidelity are paramount, (2) longitudinal studies of model competence across domains, (3) benchmarking in safe deployment contexts where misinterpretation could lead to erroneous inferences about model safety or capability, and (4) cross-study comparability when multiple teams are using MMLU as a standard metric.","general knowledge, reasoning, benchmark quality, multiple-choice, academic subjects",
MMMLU,,yes (other languages),10-30,"MMMLU is a multilingual, massively multitask language understanding benchmark designed to test broad language understanding and reasoning across a spectrum of knowledge domains featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. The base MMMLU dataset is distributed in a multilingual format, featuring professionally translated questions spanning a wide range of subjects and tasks, with an emphasis on general knowledge competencies that are not narrowly domain-bound. It is crafted to evaluate the model’s capabilities in handling language across multiple languages, maintaining consistency in question structure and answer options, and demonstrating cross-lingual comprehension, translation-invariant reasoning, and robust performance on non-English inputs. This benchmark supports cross-cultural and cross-linguistic evaluation of reasoning, memory, and general knowledge retention, making it particularly useful for assessing deployment readiness in diverse linguistic environments. Ideal for: (1) measuring multilingual general knowledge and reasoning across languages, (2) testing translation robustness and cross-lingual transfer, (3) benchmarking models intended for international or multilingual education and information access tasks, (4) comparing performance stability across languages with varying linguistic typologies and resource availability.",General knowledge,
MMMU,,,80-200,"MMMU (Massive Multi-discipline Multimodal Understanding) is a multimodal benchmark that evaluates vision-language models on exam-like questions spanning many academic subjects. It is designed to test whether a model can integrate visual information (diagrams, charts, figures, illustrations, screenshots) with domain knowledge and reasoning, producing correct answers that depend on interpreting the visual content rather than relying on language priors alone. MMMU is a strong fit when the target capability is “read and reason over a figure,” “use a diagram to solve a problem,” “interpret a chart,” or “combine image evidence with textual constraints,” especially in educational or professional contexts. In vector-search terms, MMMU matches: multimodal reasoning, diagram understanding, chart interpretation, cross-disciplinary visual QA, and exam-style VLM evaluation. Typical failure modes include ignoring key visual cues, misreading axes/labels, failing to map visual elements to concepts, and overconfident answers driven by memorized patterns rather than the image.","multimodal, vision-language, diagrams, charts, exam-style reasoning, general knowledge",MMMU Pro
MMMU,Pro,yes (multimodal),30-80,"MMMU-Pro is a stricter, harder variant of MMMU (Massive Multi-discipline Multimodal Understanding) intended to reduce shortcuts and spurious cues, pushing models toward genuine multimodal comprehension. It is useful when you suspect a baseline multimodal benchmark can be partially solved via language-only heuristics, dataset artifacts, or shallow guessing, and you want an evaluation that more reliably rewards correct visual grounding. MMMU-Pro is a strong match for tasks described as robust multimodal academic reasoning, figure-grounded problem solving, and ‘no-shortcut’ visual comprehension. In vector-search terms: robust VLM benchmarking, artifact-resistant multimodal QA, diagram-grounded reasoning, and hard exam questions with visuals. Failure modes it highlights include brittle reliance on text priors, hallucinated figure interpretation, and inability to connect multiple visual components into a single coherent solution.","multimodal, vision-language, robust evaluation, diagrams, charts, academic reasoning",
MMVet,,yes (multimodal),1-10,"MMVet is a multimodal evaluation designed to test deeper vision-language reasoning beyond simple recognition, often emphasizing compositional questions, multi-step inference, and robustness to superficial cues. It is relevant when you want a harder multimodal benchmark than standard VQA and want to stress “reasoning over the image.” Vector-search matches: hard VLM benchmark, compositional multimodal reasoning, multi-step visual inference, and robust image-grounded reasoning.","multimodal, vision-language reasoning, compositionality",
MT-bench,,,1-10,"MT-bench is a multi-turn chat benchmark designed to evaluate general assistant quality under conversational settings, emphasizing instruction-following, helpfulness, and reasoning across a dialogue rather than a single isolated prompt. The benchmark is frequently used to compare chat models on how well they maintain conversational context, respond coherently across turns, handle follow-up constraints, and produce responses that are useful and well-structured. MT-bench is especially relevant when you want a “chat-style” evaluation that approximates interactive usage: users ask, refine, challenge, and redirect; the model must remain consistent, polite, and informative. In vector-search terms it matches: conversational QA, multi-turn assistant, chat quality, dialogue coherence, context retention, and follow-up handling. Common failure modes it surfaces include context drift, contradictions across turns, shallow or generic answers, refusal inconsistencies, and inability to revise earlier claims when new constraints appear.","conversation, assistant quality, instruction following, dialogue coherence, context retention",
MedQA,,yes (medical),10-30,"MedQA is a medical question-answering benchmark that evaluates clinical knowledge and reasoning on exam-style medical questions. It targets whether models can correctly apply medical knowledge, interpret clinical scenarios, select appropriate diagnoses or treatments, and reason through differential considerations in a constrained answer format. MedQA is a strong match when you need evaluation for medical assistants, clinical decision support prototypes, medical tutoring systems, or any health-domain QA where factual correctness and medical reasoning are central. Vector-search matches: medical QA benchmark, clinical reasoning, diagnosis questions, treatment selection, exam-style medicine evaluation, and healthcare knowledge. Common failure modes include hallucinated medical facts, unsafe recommendations, confusing similar conditions, and failure to account for contraindications or clinical context details.","medicine, clinical reasoning, medical knowledge, healthcare QA",
MortgageTax,v2,yes (finance; images),10-30,"MortgageTax from Vals AI (v2) is a specialized benchmark for evaluating reading and understanding tax certificates as images. Evaluate reasoning and compliance logic around mortgage-related tax scenarios. It targets tasks where models must correctly apply tax rules and constraints to mortgage situations, handle numeric calculations, interpret eligibility/limitations, and communicate structured rationale that aligns with regulatory logic. MortgageTax is a strong fit when your task description includes: mortgage interest deduction reasoning, tax treatment of home-related expenses, eligibility determination, and scenario-based tax calculations involving property and financing. Vector-search matches: mortgage tax benchmark, housing-related tax reasoning, compliance application with numbers, and scenario-driven tax advisory. Failure modes include misapplying thresholds/eligibility, incorrect computation of deductible amounts, and reasoning that sounds confident but violates rule constraints.","tax, personal finance, mortgages, compliance reasoning, numerical reasoning",
Multi-IF,,yes (tool),80-200,"Multi-IF is a multi-turn, multilingual instruction-following benchmark explicitly introduced to evaluate how well models continue to follow instructions across *multiple turns* and across multiple languages, instead of only in single-turn English prompts. It expands upon IFEval by converting single-turn verifiable instruction prompts into multi-turn sequences and translating prompts into 7 additional languages, producing 4,501 multilingual conversations with three turns each, which makes it suitable for studying instruction drift, compounding errors, and recovery from earlier mistakes in a dialogue. Multi-IF is well-suited when your target application involves conversational assistants that must preserve constraints across a session (e.g., “keep using this schema,” “never reveal X,” “always answer in Y format,” “track a user preference”), and you want a benchmark that specifically amplifies those ‘stateful compliance’ failure modes. From a vector-search perspective, Multi-IF is a good match for tasks described as multi-step instruction retention, multilingual compliance, multi-turn formatting constraints, or “follow the same rule again later” scenarios where the benchmark should penalize partial compliance and reward robust instruction persistence.","instruction following, multi-turn dialogue, multilingual, compliance, controllability",
MultiChallenge,,yes (multi-turn chat),30-80,"MultiChallenge is a multi-turn conversation benchmark explicitly built to measure whether an assistant can survive the kinds of realistic conversational difficulties that show up in day-to-day human–LLM interactions, even when models look strong on older multi-turn chat tests. It targets four common but stubborn failure clusters: instruction retention across turns (the assistant must keep a rule alive while the user’s topic shifts), inference memory of user information (the assistant must remember and correctly reuse stable user details that were stated earlier), reliable versioned editing (the assistant must apply edits to previously produced text/code while preserving unchanged parts and honoring a new change request), and self-coherence (the assistant must remain logically consistent with its earlier statements, decisions, and commitments). A key property is that these challenges are not exotic; they are exactly what breaks agents in real deployments, because the user keeps interacting, corrects the assistant, asks for revisions, and expects persistent constraints. MultiChallenge is especially useful when you want to benchmark conversational robustness, revision discipline, instruction drift, and long-range internal consistency in a way that is closer to product usage than single-turn “follow the instruction” prompts. It matches vector-search intents like: multi-turn instruction retention, versioned editing benchmark, conversational memory of user profile, self-consistency over dialogue, revision without regression, edit-the-document-but-don’t-destroy-structure, and conversational reliability beyond surface fluency.","multi-turn dialogue, instruction retention, conversational memory, editing, self-consistency",
Natural Questions,NQ,,1-10,"Natural Questions evaluates question answering grounded in real-world information needs, often tied to long documents (such as web pages) where the model must locate relevant evidence and produce either short answers or longer explanations. It is useful when you care about open-domain QA that resembles real user questions and requires robust passage selection. Vector-search matches: open-domain QA, passage retrieval + answer extraction, long-document question answering, and evidence-grounded response generation.","open-domain QA, retrieval, reading comprehension, grounding",
OCRBench,,yes (OCR),1-10,"OCRBench is an OCR-centric benchmark that evaluates whether models can read and reason over text in images, often pushing beyond simple recognition to include layout, small fonts, and complex mixed text/graphics. It is essential when your application involves screenshots, documents, or scenes where text extraction fidelity drives downstream correctness. Vector-search matches: OCR benchmark, image text reading evaluation, screenshot understanding, and text-grounded visual reasoning.","OCR, multimodal, text in images, document understanding",
OpenBookQA,,,1-10,"OpenBookQA is a benchmark focused on elementary science questions that require combining a small “open book” of facts with additional commonsense reasoning. It targets whether a model can apply core facts and connect them to the question context rather than relying solely on memorized trivia. Vector-search matches: science reasoning with a fact set, open-book QA, fact application, and elementary science inference.","science QA, reasoning, knowledge application, multiple-choice",
PIQA,,,1-10,"PIQA evaluates physical commonsense reasoning via questions about how to accomplish everyday tasks, requiring models to select the more plausible solution among options. It targets intuitive physics and affordance reasoning, making it useful for assistants that must reason about real-world actions. Vector-search matches: physical commonsense benchmark, affordance reasoning, everyday task completion reasoning, and practical “how would you do this?” inference.","commonsense, physical reasoning, affordances, multiple-choice",
SQuAD,v2,,1-10,"SQuAD is a reading-comprehension benchmark where models answer questions based on a provided context passage, testing span extraction and context-grounded understanding. The v2 variant adds unanswerable questions, making it a benchmark for deciding when the context does not contain the answer and avoiding hallucination. Vector-search matches: context-grounded QA, extractive reading comprehension, answerability detection, and hallucination avoidance under provided evidence.","reading comprehension, extractive QA, grounding, answerability",
SVAMP,,,1-10,"SVAMP is a math word-problem benchmark that focuses on variations of standard arithmetic problems, emphasizing robustness to small textual changes that alter the underlying math. It is useful when you want to test whether a model truly understands the quantitative relationships rather than memorizing templates. Vector-search matches: robustness in math word problems, adversarially varied arithmetic stories, and sensitivity to problem phrasing changes.","mathematics, arithmetic, robustness, word problems",
SWE-bench,Verified,yes (coding),30-80,"SWE-bench Verified is a stricter variant within the SWE-bench ecosystem that is used to benchmark real-world software engineering agents on repository-level issue resolution with higher-confidence evaluation. It is designed for scenarios where you want strong trust in the reported success signal: the model must generate a patch that genuinely fixes a real issue without breaking existing behavior, validated by running tests in a reproducible harness. SWE-bench Verified is the right benchmark match for tasks described as “fix a GitHub issue in a real repo,” “generate a patch that passes tests,” “multi-file codebase edits,” “debugging under test constraints,” and “agentic SWE with reproducible evaluation.” Vector-search matches: repo-level bug fixing, test-driven patch generation, realistic software engineering benchmark, and agent evaluation for codebase modification.","software engineering, bug fixing, code editing, patch generation, test-based evaluation",
SWE-bench,,,10-30,"SWE-bench is a benchmark for evaluating language models on real-world software engineering issues collected from GitHub, where the model receives a codebase and an issue and must generate a patch that resolves the described problem. It emphasizes outcome-based verification via reproducible evaluation (Docker-based harness) and includes multiple datasets/variants (including SWE-bench Verified) to support different evaluation strictness and contamination controls. For vector-search matching, SWE-bench is a strong candidate when the task description is “fix a bug in an existing repo,” “implement a change request from an issue,” “generate a patch that makes tests pass,” “code editing across multiple files,” or “agent must work in a real repository with tests,” where success is measured by actual test outcomes rather than subjective review.","software engineering, bug fixing, code editing, patch generation, test-driven evaluation",
SciCode,,yes (coding; data science),30-80,"SciCode is a science-oriented coding benchmark that evaluates whether models can write correct code for scientific tasks, often involving domain-specific reasoning, data manipulation, numerical methods, or experimental-style computation. It is particularly relevant when you want to measure “code + science,” i.e., the ability to translate scientific intent into executable programs that produce correct results, rather than general algorithmic code alone. Vector-search terms include: scientific programming, computational science code generation, numerical methods implementation, data analysis scripts, and research-code synthesis. Typical failure modes include misuse of scientific libraries, incorrect numerical assumptions, and producing code that looks plausible but fails to compute the intended scientific quantity.","scientific coding, numerical methods, data analysis, program synthesis",
SocialIQA,,,1-10,"SocialIQA tests social commonsense reasoning, focusing on motivations, emotions, and likely reactions in interpersonal scenarios. It is relevant for assistants that need to interpret social context and choose appropriate or plausible social inferences. Vector-search matches: social reasoning benchmark, human interactions inference, motivation/emotion reasoning, and pragmatic social understanding.","social reasoning, commonsense, human behavior, multiple-choice",
Tau2-bench,Telecom,yes (multi-turn chat),30-80,"τ²-Bench (tau-squared-bench) is a conversational agent benchmark designed around a *dual-control* environment that better matches real support settings in which both the user and the agent can take actions (often via tools) that jointly change a shared world state. It is explicitly motivated by the mismatch between common single-control benchmarks (agent acts, user is passive) and real technical support, where effective solutions require coordination, communication, and guidance so that the user performs needed steps correctly. The benchmark is a good fit when your evaluation target includes ‘agent guides user actions’, ‘agent must ask for information then instruct user to do steps’, ‘coordination and communication errors’, and other interactive support patterns that are not captured by pure tool-use autonomy. For vector search: τ²-Bench matches descriptions like dual-control, user-in-the-loop tool usage, telecom troubleshooting, technical support dialogue with state transitions, and coordination-heavy resolution workflows.","conversational agents, tool use, compliance, user interaction, customer support, workflow automation, dual-control environments",
Tau-bench,,,10-30,"𝜏-bench is an agent benchmark designed around the reality that deployed assistants must coordinate among three forces at once: a human user with evolving requests, programmatic tools/APIs that change the world state, and domain-specific policies/rules that constrain what the agent is allowed to do. Instead of evaluating a conversation as a purely textual artifact, 𝜏-bench emphasizes faithful, objective success signals tied to whether the agent reached the correct goal state in the underlying environment (for example, the correct database state after performing actions), while still requiring the agent to handle messy user communication and policy constraints. The benchmark is modular by design: domains can be added, tools can be swapped, policy documents can be expanded, and tasks can be scaled, making it suitable for systematic research on agent reliability. Use this benchmark when your target system looks like “customer support agent that must use tools while following a policy,” “assistant that edits a record system under constraints,” or “agent that must ask clarifying questions, then apply tool actions correctly,” and you need an evaluation that punishes tool misuse, policy violations, and brittle handling of user ambiguity. Vector-search matches include: tool-agent-user interaction, policy-following agents, database-backed support workflows, user simulator conversations, objective end-state evaluation, and reliability under dynamic state.","conversational agents, tool use, compliance, user interaction, customer support, workflow automation",Tau2-bench
TaxEval,v2,yes (finance),10-30,"TaxEval v2 from Vals AI is a domain-specific benchmark that evaluates a model’s ability to answer hard tax-related questions, focusing on both answer correctness and structured reasoning capabilities, with questions and answers created and double-checked by financial and tax experts. It is explicitly structured to evaluate not only whether the final answer matches ground truth but also whether the stepwise reasoning is high quality and well structured, making it useful for auditing ‘correct conclusion via wrong reasoning’ versus ‘correct reasoning chain’ behavior in tax workflows. The benchmark includes diverse tax question types (e.g., numerical reasoning, compliance/application, comparative analysis, semantic analysis, updates/current affairs), which makes it suitable for systems intended for tax preparation support, tax policy Q&A, corporate tax analysis, or accounting workflows that demand both correctness and explainability. For vector-search matching, TaxEval v2 is a strong fit when the task is described as “tax law application with calculations,” “determine taxable income and rates,” “explain tax treatment of assets,” “multi-step tax reasoning,” or “professional tax advisory QA” where both the result and the reasoning trace matter.","tax, accounting, finance, numerical reasoning, regulatory compliance",
Terminal-Bench,Hard,yes (terminal use),30-80,"Terminal-Bench Hard is referenced by Artificial Analysis as an agentic benchmark evaluating AI capabilities in terminal environments through software engineering, system administration, and data processing tasks. The ‘Hard’ positioning is relevant for evaluating failure modes that only show up when an agent must plan longer, coordinate multiple command-line steps, and avoid destructive actions while still completing the task under realistic constraints. For vector-search matching, pick this when your target workload is “hard terminal agent tasks,” “multi-step CLI workflows,” “realistic sysadmin debugging,” or “terminal-based SWE tasks,” especially when you want a benchmark label that signals higher difficulty and stronger separation between shallow command emission and robust environment-driven problem solving.","terminal agents, software engineering, sysadmin, automation, tool use, environment interaction, agentic coding",
Terminal-Bench,2.0,,10-30,"Terminal-Bench 2.0 is a terminal-environment benchmark for autonomous agents that focuses on realistic technical work rather than toy command sequences. It includes a curated set of tasks that require agents to interact with actual tooling, filesystems, and codebases inside containerized environments, where the correct solution emerges from executing commands, inspecting outputs, making edits, and verifying results. The benchmark is deliberately designed as a living standard: as models improved on earlier versions, 2.0 raised the difficulty and strengthened verification so tasks remain reproducible, properly specified, and solvable without relying on fragile external dependencies. Use Terminal-Bench 2.0 when your system is a CLI-first agent (developer tooling, sysadmin assistant, data wrangler, build/debug agent) and you want a benchmark that measures action correctness, error recovery, tool competence, and end-to-end completion under environment state. Vector-search matches: terminal agent benchmark, containerized evaluation, command-line automation, debugging in shells, file manipulation with verification, multi-step operational tasks, and CI-style evaluation of agents.","terminal agents, software engineering, sysadmin, automation, tool use, environment interaction, agentic coding",
TextVQA,,yes (image reasoning),10-30,"TextVQA evaluates vision-language models on images that contain text, requiring reading embedded text (signs, labels, screenshots) and using it to answer questions. It is particularly relevant for assistants that must interpret UI screenshots, street scenes with signage, product labels, or any visual environment where text is central. Vector-search matches: OCR-in-the-wild, scene text reasoning, screenshot QA, and vision-language text grounding.","multimodal, OCR, scene text, vision-language",
ToolBench,,,1-10,"ToolBench is a tool-use benchmark focused on whether models can select and correctly use external tools/APIs to solve user tasks. It emphasizes tool selection, parameterization, sequencing across multiple tools, and robustness to tool outputs or errors. ToolBench is a strong fit for evaluating agent frameworks that rely on API calls (search, database, calendar, travel, computation) and need structured invocation behavior rather than just natural language. Vector-search matches: tool selection benchmark, API tool use evaluation, multi-tool planning, function-call correctness, and tool-augmented agent reliability.","tool use, API calling, planning, structured invocation, agents",
TriviaQA,,,1-10,"TriviaQA is an open-domain QA benchmark oriented around trivia-style questions that test broad knowledge and the ability to recover answers from evidence. It is often used as a retrieval-plus-reading benchmark, where the challenge is both finding the right document and extracting or synthesizing the correct answer. Vector-search matches: open-domain trivia QA, retrieval-augmented QA, broad factual coverage, and evidence-based answering.","open-domain QA, retrieval, factual knowledge, reading",
TruthfulQA,,,1-10,"TruthfulQA is a benchmark designed to measure whether models avoid producing common misconceptions and false but plausible answers, focusing on truthfulness in the face of misleading prompts. It is particularly relevant when you want to evaluate whether a model defaults to confident fabrication or can resist “attractive wrong answers.” Vector-search matches: truthfulness benchmark, misconception resistance, hallucination avoidance, adversarially misleading questions, and reliability of factual responses.","truthfulness, safety, factuality, hallucination resistance",
VQAv2,,,1-10,"VQAv2 is a general visual question answering benchmark that tests whether models can answer diverse questions about images, spanning objects, attributes, counting, spatial relations, and commonsense. It is often used as a baseline for broad image understanding and question answering. Vector-search matches: general VQA benchmark, image-based QA, object/attribute reasoning, and foundational vision-language evaluation.","multimodal, vision-language, image understanding, QA",
VideoMMMU,,yes (video reasoning),10-30,"VideoMMMU extends multimodal evaluation into the time dimension by benchmarking models on video-based understanding and reasoning. It targets capabilities needed for real-world video assistants: tracking events over time, understanding temporal causality, recognizing objects/actions across frames, integrating audio/text cues when present, and answering questions that require memory of earlier segments rather than only the final frame. VideoMMMU is a strong fit for tasks described as long video QA, temporal reasoning, event sequence understanding, instructional video comprehension, or multi-scene narrative tracking. In vector-search terms it matches: video understanding, temporal grounding, multi-step event inference, cross-frame memory, and spatiotemporal reasoning. Typical failure modes include focusing on a single salient frame, losing track of entity identity over time, missing key transitions, and producing plausible but ungrounded narratives.","video understanding, temporal reasoning, multimodal, event tracking, memory",
WebArena,,,1-10,"WebArena is an interactive web environment benchmark for agents that must complete tasks on realistic web applications by navigating pages, filling forms, clicking elements, and maintaining state across a browsing session. It targets the operational skill of a web-UI agent: understanding DOM-like structure, choosing actions, handling authentication-like flows, dealing with multi-step checkout or settings changes, and verifying success conditions. WebArena is a strong benchmark choice for vector-search queries like: web UI automation, browser-based agent, multi-step website tasks, form-filling agents, stateful navigation, and end-to-end task completion in a web environment. It is especially relevant when you care about action sequences and state transitions more than free-form text quality.","web agents, UI automation, browser interaction, navigation, form filling, stateful tasks",
WebShop,,,1-10,"WebShop is an agent benchmark that simulates online shopping tasks where an agent must search a product catalog, compare options, apply constraints (price, features, user preferences), and complete a purchase-like goal through a sequence of actions. It tests decision-making under constraints, iterative exploration, and preference satisfaction, making it valuable for evaluating agents that must plan and act across multiple pages and product candidates. WebShop maps well to vector-search intents such as shopping assistant evaluation, preference-based product selection, constrained decision-making agents, multi-step web navigation with selection, and goal completion in catalog environments.","agents, decision making, constrained planning, web navigation, recommendation, shopping workflows",
WinoGrande,,,1-10,"WinoGrande is a pronoun/coreference commonsense benchmark that tests whether models can resolve ambiguous references using world knowledge and context. It is commonly used to probe subtle language understanding and commonsense inference in compact text. Vector-search matches: coreference resolution benchmark, pronoun disambiguation, commonsense coreference, and context-sensitive reference resolution.","coreference, commonsense, language understanding",
WritingBench,,yes (writing),10-30,"WritingBench is a generative writing benchmark designed to evaluate high-quality writing across diverse writing intents, genres, and constraints. It targets the real requirements of writing assistants: producing text that is not just grammatically correct, but also purposeful, audience-appropriate, stylistically consistent, and compliant with format/length constraints (e.g., “write a persuasive memo with specific sections,” “produce a technical explanation with a given tone,” “generate a creative piece with structural rules”). WritingBench is especially useful when you want evaluation signals for writing craft: clarity, organization, argument structure, coherence, rhetorical effectiveness, and adherence to requested style and formatting. For vector search, it matches queries like: creative writing benchmark, persuasive writing evaluation, technical writing quality, tone control, format constraints, and long-form coherence. Failure modes it helps reveal include generic filler, weak structure, inconsistent voice, missing required components, and poor alignment to audience/goal.","writing, composition, creativity, persuasion, technical communication, style control",
