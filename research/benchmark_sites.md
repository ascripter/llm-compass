| Name / URL | Category | volume | scraping? | comment | dated |
|------------|----------|--------|-----------|---------|-------|
| [artificialanalysis.ai](https://artificialanalysis.ai/evaluations) | Multiâ€‘domain â€œhardâ€ benchmarks & cost/latency | Independent evaluations over 10+ challenging benchmarks (e.g., MMLUâ€‘Pro, GPQA Diamond, AIME, MATHâ€‘500, HLE, LiveCodeBench, SciCode, Terminalâ€‘Bench, Ï„Â²â€‘Bench), comparing 100+ models with cost and latency metrics. | ğŸ”´ forbidden | Terms of Use explicitly forbid using â€œsoftware or automated agents or scripts â€¦ to generate automated searches, requests, or queries to (or to strip, scrape, or mine data from) the Siteâ€, except for compliant public searchâ€‘engine spiders; this directly bans scraping of their leaderboards. | no |
| [LLM-Stats.com](https://llm-stats.com/benchmarks) | General multiâ€‘benchmark model stats | Aggregates benchmark scores (e.g., MMLU, GPQA, GSM8K, AIME, LiveCodeBench, SWEâ€‘bench), pricing, latency and context window data for hundreds of API models, with comparison tools and a public API. | ğŸŸ¡ not forbidden (API recommended) | Current Terms of Service describe general site use and limitations of liability but do not mention bots or automated access; the service promotes an official API for programmatic access to its aggregated data, which is preferable to HTML scraping. | no |
| [vals.ai](https://www.vals.ai/benchmarks) | Industryâ€‘specific legal / finance / tax / SWE | Private, domainâ€‘specific benchmarks on real professional workflows in law, tax, corporate finance and software engineering, built with partner firms; multiple suites (e.g., LegalBench runs, CorpFin, CaseLaw) with hundreds of realistic tasks across domains. | ğŸŸ¡ not forbidden | Public pages and benchmark descriptions do not show an explicit antiâ€‘bot clause; benchmarks are part of a commercial evaluation platform, so heavy or commercial automated use should follow contractual terms even though casual scraping is not explicitly addressed. | no |
| [Livebench.ai](https://livebench.ai/) | General LLM benchmark (multiâ€‘task, contaminationâ€‘free) | Contaminationâ€‘resistant generalâ€‘purpose benchmark with 18 tasks across 6 categories and ~1k questions, updated monthly from recent sources (math competitions, arXiv, news, IMDb, new datasets) and scored by objective ground truth. | ğŸŸ¡ not forbidden | Benchmark code and questions are openâ€‘sourced under permissive licenses via the LiveBench GitHub and associated datasets; the public leaderboard is backed by these OSS artifacts, so automated access is best done via GitHub/Hugging Face rather than heavy scraping of the live site. | no |
| [Terminal Bench 2.0](https://www.tbench.ai/leaderboard/terminal-bench/2.0) | Terminal / CLI agent benchmark (toolâ€‘using coding + ops agents) | terminal-bench@2.0: 89 curated, sandboxed terminal tasks; public leaderboard shows 100+ agent/model entries. | ğŸŸ¡ not forbidden | Terminalâ€‘Bench is described as a collection of tasks plus an evaluation harness to quantify agentsâ€™ terminal mastery; Terminalâ€‘Bench 2.0 specifically raised difficulty and improved task reproducibility/verification for agent evaluation. | no |
| [Berkeley Function Calling Leaderboard (BFCL)](https://gorilla.cs.berkeley.edu/leaderboard) | Function calling / tool invocation | Academic leaderboard for Gorillaâ€™s functionâ€‘calling benchmarks, covering many APIs and programming libraries with structured questionâ€“functionâ€“answer pairs and functionâ€‘call accuracy metrics across numerous models. | ğŸŸ¡ not forbidden | Underlying Gorilla/OpenFunctions datasets use open licenses; the Berkeley project site itself is a standard academic page with no explicit robots/scraping clause, so automated access is not clearly prohibited but should remain moderate. | somewhat |
| [MCP Mark](https://mcpmark.ai/) | MCP / toolâ€‘use agent benchmark | Stressâ€‘tests MCPâ€‘based agents across multiple environments (e.g., Notion, GitHub, filesystem, PostgreSQL, Playwright), with realistic multiâ€‘step tasks and an evaluation harness that checks tool use, success and robustness. | ğŸŸ¡ not forbidden | The benchmark is described in an academic paper and accompanying code; public materials and the main task site do not publish specific antiâ€‘scraping terms, though usage of any included thirdâ€‘party services must still respect their own licenses. | somewhat |
| [SWE-bench (Bash Only)](https://www.swebench.com/bash-only.html) | Software engineering (bashâ€‘only agents) | Uses the 500â€‘sample SWEâ€‘bench Verified subset in a shared minimal â€œbashâ€‘onlyâ€ miniâ€‘SWEâ€‘agent environment; leaderboard reports % Resolved on these real GitHub issues under a standardized ReAct bash loop. | ğŸŸ¢ allowed | SWEâ€‘bench datasets and evaluation harnesses are MITâ€‘licensed, and the Bashâ€‘Only page documents a reproducible configuration and versioned setup; there is no separate antiâ€‘scraping language on the site beyond normal academic use. | somewhat |
| [GSO Leaderboard](https://gso-bench.github.io/) | Software performance optimization | Global Software Optimization benchmark: 102 challenging optimization tasks across 10 realâ€‘world codebases and several languages (Python, C, C++, SIMD, Rust, Cython), evaluated via OPT@K against expertâ€‘engineer speedups. | ğŸŸ¢ allowed | Core benchmark and leaderboard data are MITâ€‘licensed via the gsoâ€‘bench/gso GitHub repo and a Hugging Face dataset; programmatic download via those channels is explicitly supported, and the static leaderboard site has no separate antiâ€‘robot terms. | somewhat |
| [Scale AI Leaderboards](https://scale.com/leaderboard) | Multiâ€‘domain LLM & agent benchmarks | Suite of leaderboards (math, SWEâ€‘bench Pro, agents, etc.) based on Scaleâ€‘run evaluations of many closed and open models on proprietary and curated tasks. | ğŸ”´ forbidden | Scaleâ€™s service/website terms state that the license to use the services â€œdoes not include â€¦ use of data mining, robots or similar data gathering and extraction toolsâ€ and that no portion may be reproduced without consent.â€‹ Automated scraping of the leaderboard conflicts with these terms. | somewhat |
| [Opencompass (chinese)](https://rank.opencompass.org.cn/leaderboard-llm) | General multiâ€‘domain LLM & LVLM evaluation | CompassRank leaderboard for the OpenCompass suite: >70â€“100 benchmarks (~400k questions) across language, knowledge, reasoning, math, coding and instruction following, aggregating scores for many open and commercial models. | ğŸŸ¢ allowed | The underlying OpenCompass toolkit is Apacheâ€‘2.0 licensed and supports fully reproducible evaluations; the leaderboard is a communityâ€‘run ranking site without published antiâ€‘scraping rules, and bulk data is obtainable via the openâ€‘source evaluation logs and configs. | somewhat |
| [BigCodeBench Leaderboard](https://bigcode-bench.github.io) | Code generation | 1,140 functionâ€‘level tasks across 139 libraries; evaluates BigCodeBenchâ€‘Complete (docstringâ€‘based completion) and BigCodeBenchâ€‘Instruct (instructionâ€‘style prompts) using calibrated Pass@1 and Elo. | ğŸŸ¢ allowed | BigCodeBench dataset and tools are under Apacheâ€‘2.0 on GitHub/Hugging Face, which explicitly allow copying and redistribution. Site is static GitHub Pages; no separate antiâ€‘robot terms located. | yes |
| [Uâ€‘MATH & Î¼â€‘MATH (Toloka math benchmark)](https://toloka.ai/math-benchmark) | Universityâ€‘level mathematics | Benchmarks LLMs on challenging universityâ€‘level math, with Uâ€‘MATH and Î¼â€‘MATH datasets; focuses on proofâ€‘style and quantitative reasoning beyond standard competition problems.â€‹ | ğŸŸ¢ allowed (subject to robots.txt) | Toloka Terms of Use explicitly: â€œscrape or crawl except as allowed by robots.txtâ€.â€‹ Automated crawling is contractually allowed if it respects toloka.aiâ€™s robots.txt; other scraping patterns are disallowed. | yes |
| [Aider](https://aider.chat/docs/leaderboards/) | Code editing (polyglot coding) | Quantitative leaderboards of LLM codeâ€‘editing skill: the classic benchmark uses 133 Exercism Python exercises; the newer polyglot leaderboard evaluates models on a larger multilingual coding suite and reports % tasks completed plus cost. | ğŸŸ¢ allowed | Aider is an openâ€‘source project (MITâ€‘licensed) and its leaderboards are documented on static docs pages backed by public benchmark data files; no antiâ€‘robot clauses are published, and the data can be accessed directly from the associated GitHub repo instead of scraping HTML. | yes |
